{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Validation Pipeline Component\n",
    "\n",
    "This notebook demonstrates comprehensive model evaluation and validation as part of the Elyra MLOps pipeline.\n",
    "It performs advanced model analysis and prepares models for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "best_model_path = \"/tmp/models/best_model.pkl\"\n",
    "test_data_path = \"/tmp/test_data.csv\"\n",
    "model_metadata_path = \"/tmp/models/model_metadata.json\"\n",
    "evaluation_output_path = \"/tmp/evaluation\"\n",
    "performance_threshold = 0.8  # Minimum F1 score for production deployment\n",
    "model_registry_path = \"/tmp/model_registry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(evaluation_output_path, exist_ok=True)\n",
    "os.makedirs(model_registry_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_data():\n",
    "    \"\"\"Load the trained model and test data.\"\"\"\n",
    "    try:\n",
    "        # Load model\n",
    "        model = joblib.load(best_model_path)\n",
    "        logger.info(f\"Model loaded: {type(model).__name__}\")\n",
    "        \n",
    "        # Load test data\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "        X_test = test_df.drop('target', axis=1)\n",
    "        y_test = test_df['target']\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(model_metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Test data shape: {X_test.shape}\")\n",
    "        \n",
    "        return model, X_test, y_test, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model and data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load everything\n",
    "model, X_test, y_test, metadata = load_model_and_data()\n",
    "\n",
    "print(f\"Model Type: {type(model).__name__}\")\n",
    "print(f\"Features: {list(X_test.columns)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Model metadata: {metadata['best_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(model, X_test, y_test):\n",
    "    \"\"\"Perform comprehensive model evaluation.\"\"\"\n",
    "    \n",
    "    # Basic predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "        'precision': float(precision_score(y_test, y_pred)),\n",
    "        'recall': float(recall_score(y_test, y_pred)),\n",
    "        'f1_score': float(f1_score(y_test, y_pred)),\n",
    "        'roc_auc': float(roc_auc_score(y_test, y_pred_proba))\n",
    "    }\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC curve data\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Precision-Recall curve data\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    evaluation_results = {\n",
    "        'metrics': metrics,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'roc_curve': {\n",
    "            'fpr': fpr.tolist(),\n",
    "            'tpr': tpr.tolist(),\n",
    "            'thresholds': roc_thresholds.tolist()\n",
    "        },\n",
    "        'precision_recall_curve': {\n",
    "            'precision': precision_curve.tolist(),\n",
    "            'recall': recall_curve.tolist(),\n",
    "            'thresholds': pr_thresholds.tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return evaluation_results, y_pred, y_pred_proba\n",
    "\n",
    "# Perform evaluation\n",
    "eval_results, y_pred, y_pred_proba = comprehensive_evaluation(model, X_test, y_test)\n",
    "\n",
    "print(\"Comprehensive Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in eval_results['metrics'].items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(np.array(eval_results['confusion_matrix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_performance(eval_results, threshold=0.8):\n",
    "    \"\"\"Validate model meets performance requirements.\"\"\"\n",
    "    \n",
    "    f1_score = eval_results['metrics']['f1_score']\n",
    "    accuracy = eval_results['metrics']['accuracy']\n",
    "    roc_auc = eval_results['metrics']['roc_auc']\n",
    "    \n",
    "    validation_results = {\n",
    "        'performance_threshold': threshold,\n",
    "        'meets_f1_threshold': f1_score >= threshold,\n",
    "        'meets_accuracy_threshold': accuracy >= 0.75,  # Secondary threshold\n",
    "        'meets_auc_threshold': roc_auc >= 0.75,  # Secondary threshold\n",
    "        'overall_validation': False\n",
    "    }\n",
    "    \n",
    "    # Overall validation\n",
    "    validation_results['overall_validation'] = (\n",
    "        validation_results['meets_f1_threshold'] and\n",
    "        validation_results['meets_accuracy_threshold'] and\n",
    "        validation_results['meets_auc_threshold']\n",
    "    )\n",
    "    \n",
    "    # Performance status\n",
    "    if validation_results['overall_validation']:\n",
    "        status = \"APPROVED_FOR_PRODUCTION\"\n",
    "        logger.info(\"Model validation PASSED - approved for production\")\n",
    "    else:\n",
    "        status = \"REQUIRES_IMPROVEMENT\"\n",
    "        logger.warning(\"Model validation FAILED - requires improvement\")\n",
    "    \n",
    "    validation_results['deployment_status'] = status\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate performance\n",
    "validation = validate_model_performance(eval_results, performance_threshold)\n",
    "\n",
    "print(\"\\nModel Performance Validation:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Performance Threshold: {validation['performance_threshold']}\")\n",
    "print(f\"F1 Score Check: {'✓' if validation['meets_f1_threshold'] else '✗'}\")\n",
    "print(f\"Accuracy Check: {'✓' if validation['meets_accuracy_threshold'] else '✗'}\")\n",
    "print(f\"ROC AUC Check: {'✓' if validation['meets_auc_threshold'] else '✗'}\")\n",
    "print(f\"\\nDeployment Status: {validation['deployment_status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model_version(model, eval_results, validation, registry_path):\n",
    "    \"\"\"Register model version in model registry.\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_version = f\"v_{timestamp}\"\n",
    "    \n",
    "    # Create version directory\n",
    "    version_dir = f\"{registry_path}/{model_version}\"\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"{version_dir}/model.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    # Create comprehensive model card\n",
    "    model_card = {\n",
    "        'model_info': {\n",
    "            'version': model_version,\n",
    "            'model_type': type(model).__name__,\n",
    "            'training_timestamp': metadata.get('timestamp', 'unknown'),\n",
    "            'evaluation_timestamp': timestamp,\n",
    "            'model_parameters': model.get_params() if hasattr(model, 'get_params') else {}\n",
    "        },\n",
    "        'performance': eval_results['metrics'],\n",
    "        'validation': validation,\n",
    "        'data_info': {\n",
    "            'features': list(X_test.columns),\n",
    "            'n_features': len(X_test.columns),\n",
    "            'test_samples': len(X_test)\n",
    "        },\n",
    "        'deployment_info': {\n",
    "            'ready_for_deployment': validation['overall_validation'],\n",
    "            'deployment_status': validation['deployment_status'],\n",
    "            'recommended_for_production': validation['overall_validation']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save model card\n",
    "    model_card_path = f\"{version_dir}/model_card.json\"\n",
    "    with open(model_card_path, 'w') as f:\n",
    "        json.dump(model_card, f, indent=2)\n",
    "    \n",
    "    # Update registry index\n",
    "    registry_index_path = f\"{registry_path}/registry_index.json\"\n",
    "    \n",
    "    if os.path.exists(registry_index_path):\n",
    "        with open(registry_index_path, 'r') as f:\n",
    "            registry_index = json.load(f)\n",
    "    else:\n",
    "        registry_index = {'versions': []}\n",
    "    \n",
    "    # Add new version\n",
    "    version_entry = {\n",
    "        'version': model_version,\n",
    "        'timestamp': timestamp,\n",
    "        'f1_score': eval_results['metrics']['f1_score'],\n",
    "        'deployment_status': validation['deployment_status'],\n",
    "        'model_path': model_path,\n",
    "        'model_card_path': model_card_path\n",
    "    }\n",
    "    \n",
    "    registry_index['versions'].append(version_entry)\n",
    "    \n",
    "    # Sort by timestamp (newest first)\n",
    "    registry_index['versions'].sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "    \n",
    "    # Update latest production model if this one is approved\n",
    "    if validation['overall_validation']:\n",
    "        registry_index['latest_production'] = version_entry\n",
    "    \n",
    "    # Save updated index\n",
    "    with open(registry_index_path, 'w') as f:\n",
    "        json.dump(registry_index, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Model version {model_version} registered successfully\")\n",
    "    \n",
    "    return {\n",
    "        'version': model_version,\n",
    "        'version_dir': version_dir,\n",
    "        'model_path': model_path,\n",
    "        'model_card_path': model_card_path,\n",
    "        'registry_index_path': registry_index_path\n",
    "    }\n",
    "\n",
    "# Register model version\n",
    "registry_info = register_model_version(\n",
    "    model, eval_results, validation, model_registry_path\n",
    ")\n",
    "\n",
    "print(\"Model Registration Completed:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model Version: {registry_info['version']}\")\n",
    "print(f\"Registry Path: {registry_info['version_dir']}\")\n",
    "print(f\"Model Card: {registry_info['model_card_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(eval_results, validation, registry_info):\n",
    "    \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'evaluation_summary': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': registry_info['version'],\n",
    "            'model_type': type(model).__name__,\n",
    "            'overall_status': validation['deployment_status']\n",
    "        },\n",
    "        'performance_metrics': eval_results['metrics'],\n",
    "        'validation_results': validation,\n",
    "        'deployment_recommendation': {\n",
    "            'ready_for_production': validation['overall_validation'],\n",
    "            'confidence_level': 'HIGH' if validation['overall_validation'] else 'LOW',\n",
    "            'next_steps': [\n",
    "                \"Deploy to KServe\" if validation['overall_validation'] else \"Improve model performance\",\n",
    "                \"Set up monitoring\",\n",
    "                \"Configure auto-scaling\"\n",
    "            ] if validation['overall_validation'] else [\n",
    "                \"Retrain with more data\",\n",
    "                \"Feature engineering\",\n",
    "                \"Hyperparameter tuning\"\n",
    "            ]\n",
    "        },\n",
    "        'model_registry': {\n",
    "            'version': registry_info['version'],\n",
    "            'model_path': registry_info['model_path'],\n",
    "            'model_card': registry_info['model_card_path']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = f\"{evaluation_output_path}/evaluation_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Generate report\n",
    "report_path = generate_evaluation_report(eval_results, validation, registry_info)\n",
    "\n",
    "print(\"Evaluation Report Generated:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Report saved to: {report_path}\")\n",
    "print(f\"\\nDeployment Recommendation: {'PROCEED' if validation['overall_validation'] else 'IMPROVE MODEL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline outputs for deployment\n",
    "pipeline_outputs = {\n",
    "    'evaluation_report': report_path,\n",
    "    'model_registry_version': registry_info['version'],\n",
    "    'production_ready_model': registry_info['model_path'],\n",
    "    'model_card': registry_info['model_card_path'],\n",
    "    'deployment_approved': validation['overall_validation'],\n",
    "    'deployment_status': validation['deployment_status'],\n",
    "    'model_performance': eval_results['metrics'],\n",
    "    'registry_index': registry_info['registry_index_path']\n",
    "}\n",
    "\n",
    "print(\"Model Evaluation Pipeline Component Completed Successfully!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel Version: {registry_info['version']}\")\n",
    "print(f\"Deployment Status: {validation['deployment_status']}\")\n",
    "print(f\"Production Ready: {'Yes' if validation['overall_validation'] else 'No'}\")\n",
    "\n",
    "print(\"\\nKey Performance Metrics:\")\n",
    "for metric, value in eval_results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline Outputs:\")\n",
    "for key, value in pipeline_outputs.items():\n",
    "    if isinstance(value, str) and value.startswith('/'):\n",
    "        exists = os.path.exists(value)\n",
    "        print(f\"  {key}: {'✓' if exists else '✗'} {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "if validation['overall_validation']:\n",
    "    print(\"  ✓ Model ready for KServe deployment\")\n",
    "    print(\"  ✓ Proceed to deployment pipeline\")\n",
    "    print(\"  ✓ Set up monitoring and alerting\")\n",
    "else:\n",
    "    print(\"  ⚠ Model requires improvement before deployment\")\n",
    "    print(\"  ⚠ Consider retraining or feature engineering\")\n",
    "    print(\"  ⚠ Review performance thresholds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}