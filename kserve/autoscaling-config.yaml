apiVersion: v1
kind: ConfigMap
metadata:
  name: kserve-autoscaling-config
  namespace: kserve-inference
data:
  autoscaling.yaml: |
    # KServe Autoscaling Configuration for Elyra Demo
    
    # Knative Serving Autoscaling
    autoscaling:
      # Serverless autoscaling
      knative:
        minScale: 0  # Scale to zero when idle
        maxScale: 20  # Maximum replicas
        target: 5  # Target concurrent requests per replica
        targetUtilizationPercentage: 70  # CPU utilization target
        scaleDownDelay: "30s"  # Delay before scaling down
        scaleUpDelay: "10s"   # Delay before scaling up
        stableWindow: "60s"   # Stable window for autoscaling decisions
        
      # HPA Configuration for non-serverless deployments
      hpa:
        enabled: true
        minReplicas: 1
        maxReplicas: 15
        targetCPUUtilizationPercentage: 70
        targetMemoryUtilizationPercentage: 80
        metrics:
          - type: Resource
            resource:
              name: cpu
              target:
                type: Utilization
                averageUtilization: 70
          - type: Resource
            resource:
              name: memory
              target:
                type: Utilization
                averageUtilization: 80
          - type: Pods
            pods:
              metric:
                name: requests_per_second
              target:
                type: AverageValue
                averageValue: "10"
        
      # Custom metrics for ML workloads
      custom_metrics:
        - name: "model_inference_queue_length"
          target_value: 5
          metric_type: "queue"
        - name: "gpu_utilization"
          target_value: 75
          metric_type: "resource"
        - name: "model_prediction_latency_p95"
          target_value: 500  # milliseconds
          metric_type: "latency"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: elyra-model-hpa
  namespace: kserve-inference
spec:
  scaleTargetRef:
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    name: elyra-demo-model
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: inference_requests_per_second
      target:
        type: AverageValue
        averageValue: "10"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60   # 1 minute
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
  namespace: kserve-inference
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "kserve_rules.yml"
    
    scrape_configs:
    - job_name: 'kserve-inference'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - kserve-inference
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
  
  kserve_rules.yml: |
    groups:
    - name: kserve.rules
      rules:
      - alert: HighInferenceLatency
        expr: histogram_quantile(0.95, rate(kserve_model_inference_duration_seconds_bucket[5m])) > 1.0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High inference latency detected"
          description: "Model {{ $labels.model_name }} has high latency: {{ $value }}s"
      
      - alert: HighErrorRate
        expr: rate(kserve_model_inference_errors_total[5m]) / rate(kserve_model_inference_requests_total[5m]) > 0.05
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Model {{ $labels.model_name }} error rate: {{ $value | humanizePercentage }}"
      
      - alert: ModelDown
        expr: up{job="kserve-inference"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Model serving is down"
          description: "Model {{ $labels.model_name }} is not responding"