apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-monitoring-config
  namespace: elyra-demo-project
data:
  prometheus_config.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
    - job_name: 'vllm-metrics'
      static_configs:
      - targets: ['vllm-service:8081']
      metrics_path: /metrics
      scrape_interval: 10s
      
    - job_name: 'nvidia-dcgm-metrics'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - nvidia-gpu-operator
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: nvidia-dcgm-exporter
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)
        replacement: ${1}:9400
      
    rule_files:
    - "/etc/prometheus/vllm_rules.yml"
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
  
  vllm_rules.yml: |
    groups:
    - name: vllm.rules
      rules:
      # Throughput and Performance Rules
      - record: vllm:request_rate_5m
        expr: rate(vllm_request_total[5m])
      
      - record: vllm:avg_latency_5m
        expr: rate(vllm_request_duration_seconds_sum[5m]) / rate(vllm_request_duration_seconds_count[5m])
      
      - record: vllm:tokens_per_second_5m
        expr: rate(vllm_output_tokens_total[5m])
      
      - record: vllm:gpu_utilization
        expr: nvidia_gpu_utilization_gpu
      
      - record: vllm:gpu_memory_used_percent
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100
      
      # Alert Rules
      - alert: VLLMHighLatency
        expr: vllm:avg_latency_5m > 5.0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "vLLM inference latency is high"
          description: "Average latency over 5m is {{ $value }}s"
      
      - alert: VLLMHighErrorRate
        expr: rate(vllm_request_errors_total[5m]) / rate(vllm_request_total[5m]) > 0.05
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "vLLM error rate is high"
          description: "Error rate is {{ $value | humanizePercentage }}"
      
      - alert: VLLMLowThroughput
        expr: vllm:request_rate_5m < 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "vLLM throughput is low"
          description: "Request rate is {{ $value }} requests/sec"
      
      - alert: VLLMGPUMemoryHigh
        expr: vllm:gpu_memory_used_percent > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "vLLM GPU memory usage is critical"
          description: "GPU memory usage is {{ $value }}%"
      
      - alert: VLLMServiceDown
        expr: up{job="vllm-metrics"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "vLLM service is down"
          description: "vLLM inference service is not responding"
      
      - alert: VLLMQueueDepthHigh
        expr: vllm_queue_depth > 100
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "vLLM request queue is deep"
          description: "Queue depth is {{ $value }} requests"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-vllm-dashboard
  namespace: elyra-demo-project
  labels:
    grafana_dashboard: "1"
data:
  vllm-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "vLLM Performance Dashboard",
        "tags": ["vllm", "llm", "inference"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(vllm_request_total[5m])",
                "legendFormat": "Requests/sec"
              }
            ],
            "yAxes": [
              {
                "label": "Requests per second"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Average Latency",
            "type": "graph",
            "targets": [
              {
                "expr": "vllm:avg_latency_5m",
                "legendFormat": "Avg Latency (s)"
              },
              {
                "expr": "histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket[5m]))",
                "legendFormat": "P95 Latency (s)"
              },
              {
                "expr": "histogram_quantile(0.99, rate(vllm_request_duration_seconds_bucket[5m]))",
                "legendFormat": "P99 Latency (s)"
              }
            ],
            "yAxes": [
              {
                "label": "Seconds"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
          },
          {
            "id": 3,
            "title": "Tokens per Second",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(vllm_output_tokens_total[5m])",
                "legendFormat": "Output Tokens/sec"
              },
              {
                "expr": "rate(vllm_input_tokens_total[5m])",
                "legendFormat": "Input Tokens/sec"
              }
            ],
            "yAxes": [
              {
                "label": "Tokens per second"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
          },
          {
            "id": 4,
            "title": "GPU Utilization",
            "type": "graph",
            "targets": [
              {
                "expr": "DCGM_FI_DEV_GPU_UTIL",
                "legendFormat": "GPU {{gpu}} Utilization %"
              }
            ],
            "yAxes": [
              {
                "label": "Percentage",
                "max": 100,
                "min": 0
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
          },
          {
            "id": 5,
            "title": "GPU Memory Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "DCGM_FI_DEV_FB_USED / 1024 / 1024",
                "legendFormat": "GPU {{gpu}} Memory Used (MB)"
              },
              {
                "expr": "DCGM_FI_DEV_FB_TOTAL / 1024 / 1024", 
                "legendFormat": "GPU {{gpu}} Memory Total (MB)"
              }
            ],
            "yAxes": [
              {
                "label": "Megabytes"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
          },
          {
            "id": 6,
            "title": "Request Queue Depth",
            "type": "graph",
            "targets": [
              {
                "expr": "vllm_queue_depth",
                "legendFormat": "Queue Depth"
              }
            ],
            "yAxes": [
              {
                "label": "Number of requests"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
          },
          {
            "id": 7,
            "title": "Error Rate",
            "type": "singlestat",
            "targets": [
              {
                "expr": "rate(vllm_request_errors_total[5m]) / rate(vllm_request_total[5m]) * 100",
                "legendFormat": "Error Rate %"
              }
            ],
            "valueFontSize": "80%",
            "valueName": "current",
            "thresholds": "1,5",
            "colorBackground": true,
            "gridPos": {"h": 4, "w": 6, "x": 0, "y": 24}
          },
          {
            "id": 8,
            "title": "Active Connections",
            "type": "singlestat",
            "targets": [
              {
                "expr": "vllm_active_connections",
                "legendFormat": "Active Connections"
              }
            ],
            "valueFontSize": "80%",
            "valueName": "current",
            "gridPos": {"h": 4, "w": 6, "x": 6, "y": 24}
          },
          {
            "id": 9,
            "title": "Model Loading Status",
            "type": "singlestat",
            "targets": [
              {
                "expr": "vllm_model_loaded",
                "legendFormat": "Model Loaded"
              }
            ],
            "valueFontSize": "80%",
            "valueName": "current",
            "valueMaps": [
              {"value": "0", "text": "Not Loaded"},
              {"value": "1", "text": "Loaded"}
            ],
            "gridPos": {"h": 4, "w": 6, "x": 12, "y": 24}
          },
          {
            "id": 10,
            "title": "Cache Hit Rate",
            "type": "singlestat",
            "targets": [
              {
                "expr": "rate(vllm_cache_hits_total[5m]) / (rate(vllm_cache_hits_total[5m]) + rate(vllm_cache_misses_total[5m])) * 100",
                "legendFormat": "Cache Hit Rate %"
              }
            ],
            "valueFontSize": "80%",
            "valueName": "current",
            "thresholds": "50,80",
            "colorBackground": true,
            "gridPos": {"h": 4, "w": 6, "x": 18, "y": 24}
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "10s"
      }
    }
# GPU metrics are provided by NVIDIA DCGM Exporter (part of NVIDIA Operator)
# No need for custom GPU exporter when NVIDIA Operator is installed
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-monitoring
  namespace: elyra-demo-project
  labels:
    app: vllm-monitoring
spec:
  selector:
    matchLabels:
      app: vllm-llama-model
  endpoints:
  - port: metrics
    interval: 10s
    path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor  
metadata:
  name: nvidia-dcgm-monitoring
  namespace: elyra-demo-project
  labels:
    app: nvidia-dcgm-monitoring
spec:
  namespaceSelector:
    matchNames:
    - nvidia-gpu-operator
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics