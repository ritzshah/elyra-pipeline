apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: vllm-llama-model
  namespace: elyra-demo-project
  annotations:
    serving.kserve.io/deploymentMode: "RawDeployment"
    serving.kserve.io/autoscalerClass: "hpa"
spec:
  predictor:
    serviceAccountName: kserve-sa
    containers:
    - name: vllm-container
      image: vllm/vllm-openai:latest
      command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
      args:
      - --model=/models/llama-2-7b-chat
      - --host=0.0.0.0
      - --port=8080
      - --tensor-parallel-size=1
      - --gpu-memory-utilization=0.85  # Conservative for T4
      - --max-model-len=2048           # Reduced for T4 memory
      - --max-num-seqs=128             # Optimized for T4
      - --enable-prefix-caching
      - --disable-log-stats
      - --trust-remote-code
      - --dtype=half                   # Use FP16 for memory efficiency
      env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: huggingface-secret
            key: token
      - name: VLLM_LOGGING_LEVEL
        value: "INFO"
      ports:
      - containerPort: 8080
        protocol: TCP
      resources:
        requests:
          nvidia.com/gpu: 1
          cpu: "4"
          memory: "16Gi"
        limits:
          nvidia.com/gpu: 1
          cpu: "8"
          memory: "32Gi"
      volumeMounts:
      - name: model-storage
        mountPath: /models
      - name: cache-volume
        mountPath: /root/.cache
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 180
        periodSeconds: 10
        timeoutSeconds: 30
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 300
        periodSeconds: 30
        timeoutSeconds: 30
    volumes:
    - name: model-storage
      persistentVolumeClaim:
        claimName: llm-models-pvc
    - name: cache-volume
      emptyDir:
        sizeLimit: 10Gi
    nodeSelector:
      accelerator: nvidia-tesla-t4
    tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "gpu-workload"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-models-pvc
  namespace: elyra-demo-project
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: fast-ssd
---
apiVersion: v1
kind: Secret
metadata:
  name: huggingface-secret
  namespace: elyra-demo-project
type: Opaque
data:
  token: ""  # Base64 encoded HuggingFace token
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
  namespace: elyra-demo-project
  labels:
    app: vllm-llama-model
spec:
  selector:
    app: vllm-llama-model-predictor
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  type: ClusterIP